{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Running on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "# training + validation dataset\n",
    "training_set_full = datasets.MNIST('dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# test dataset\n",
    "test_set = datasets.MNIST('dataset/', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size after splitting to train / validation / test:\n",
      "Training data: 54000, Sample structure: (<class 'torch.Tensor'> torch.Size([1, 28, 28]), <class 'int'>)\n",
      "Validation data: 6000, Sample structure: (<class 'torch.Tensor'> torch.Size([1, 28, 28]), <class 'int'>)\n",
      "Test data: 10000, Sample structure: (<class 'torch.Tensor'> torch.Size([1, 28, 28]), <class 'int'>)\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_FRAC = 0.1\n",
    "SAMPLE_IMG_ID=0\n",
    "\n",
    "np.random.seed(42)\n",
    "shuffled_idx = np.random.choice(len(training_set_full), len(training_set_full), replace=False)\n",
    "validation_set = data_utils.Subset(training_set_full, shuffled_idx[:int(len(training_set_full) * 0.1)])\n",
    "training_set = data_utils.Subset(training_set_full, shuffled_idx[int(len(training_set_full) * 0.1):])\n",
    "\n",
    "print('Data size after splitting to train / validation / test:')\n",
    "print(f'Training data: {len(training_set)}, Sample structure: ({type(training_set[SAMPLE_IMG_ID][0])} {training_set[SAMPLE_IMG_ID][0].shape}, {type(training_set[SAMPLE_IMG_ID][1])})')\n",
    "print(f'Validation data: {len(validation_set)}, Sample structure: ({type(validation_set[SAMPLE_IMG_ID][0])} {validation_set[SAMPLE_IMG_ID][0].shape}, {type(validation_set[SAMPLE_IMG_ID][1])})')\n",
    "print(f'Test data: {len(test_set)}, Sample structure: ({type(test_set[SAMPLE_IMG_ID][0])} {test_set[SAMPLE_IMG_ID][0].shape}, {type(test_set[SAMPLE_IMG_ID][1])})')\n",
    "\n",
    "training_loader = DataLoader(dataset=training_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(dataset=validation_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = training_set_full[0][0].shape[1], training_set_full[0][0].shape[2]\n",
    "classes = np.unique([t[1] for t in training_set_full])\n",
    "NUM_CLASSES = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, classes):\n",
    "        super().__init__()\n",
    "        # layer 1\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        # layer 2\n",
    "        self.fc2 = nn.Linear(50, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # run x through the layers and activation functions\n",
    "        # (relu activation function is just max(0, x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # normally there's no activation function on last layer (except softmax etc. when needed)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleNeuralNetwork(np.product(IMAGE_SHAPE), NUM_CLASSES)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_function, X, y):\n",
    "    # flatten the data from (batch_size, W, H) to (batch_size, W*H) because pytorch models require 1 dimensional samples, and run model.\n",
    "    predictions = model(X.reshape(X.shape[0], -1))  # Module class implements the __call__ method. that's why this class instance is callable although it's not a function.\n",
    "                                                    # its __call__ method simply calls our forward() method.\n",
    "    \n",
    "    loss = loss_function(predictions, y)\n",
    "    predictions = predictions.argmax(dim=1).cpu().numpy()\n",
    "    acc = (predictions == y.cpu().numpy()).mean()\n",
    "    return predictions, acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training acc: 0.00, Training loss: 2.30, Validation acc: 0.13, Validation loss: 2.30\n",
      "Training acc: 0.84, Training loss: 0.53, Validation acc: 0.91, Validation loss: 0.31\n",
      "Training acc: 0.90, Training loss: 0.31, Validation acc: 0.93, Validation loss: 0.26\n",
      "Training acc: 0.93, Training loss: 0.24, Validation acc: 0.93, Validation loss: 0.24\n",
      "Training acc: 0.94, Training loss: 0.22, Validation acc: 0.94, Validation loss: 0.22\n",
      "Training acc: 0.93, Training loss: 0.22, Validation acc: 0.94, Validation loss: 0.18\n",
      "Training acc: 0.94, Training loss: 0.19, Validation acc: 0.94, Validation loss: 0.20\n",
      "Training acc: 0.94, Training loss: 0.20, Validation acc: 0.95, Validation loss: 0.16\n",
      "Training acc: 0.95, Training loss: 0.18, Validation acc: 0.95, Validation loss: 0.16\n",
      "Training acc: 0.95, Training loss: 0.18, Validation acc: 0.95, Validation loss: 0.17\n",
      "Epoch 2\n",
      "Training acc: 1.00, Training loss: 0.00, Validation acc: 0.96, Validation loss: 0.14\n",
      "Training acc: 0.96, Training loss: 0.15, Validation acc: 0.96, Validation loss: 0.15\n",
      "Training acc: 0.96, Training loss: 0.14, Validation acc: 0.95, Validation loss: 0.17\n",
      "Training acc: 0.96, Training loss: 0.14, Validation acc: 0.96, Validation loss: 0.16\n",
      "Training acc: 0.96, Training loss: 0.14, Validation acc: 0.96, Validation loss: 0.15\n",
      "Training acc: 0.96, Training loss: 0.14, Validation acc: 0.95, Validation loss: 0.15\n",
      "Training acc: 0.96, Training loss: 0.15, Validation acc: 0.95, Validation loss: 0.18\n",
      "Training acc: 0.96, Training loss: 0.13, Validation acc: 0.96, Validation loss: 0.15\n",
      "Training acc: 0.97, Training loss: 0.14, Validation acc: 0.96, Validation loss: 0.16\n",
      "Training acc: 0.96, Training loss: 0.13, Validation acc: 0.96, Validation loss: 0.14\n",
      "Epoch 3\n",
      "Training acc: 1.00, Training loss: 0.00, Validation acc: 0.96, Validation loss: 0.13\n",
      "Training acc: 0.97, Training loss: 0.11, Validation acc: 0.96, Validation loss: 0.14\n",
      "Training acc: 0.96, Training loss: 0.12, Validation acc: 0.97, Validation loss: 0.14\n",
      "Training acc: 0.97, Training loss: 0.12, Validation acc: 0.96, Validation loss: 0.15\n",
      "Training acc: 0.97, Training loss: 0.11, Validation acc: 0.96, Validation loss: 0.15\n",
      "Training acc: 0.96, Training loss: 0.13, Validation acc: 0.95, Validation loss: 0.18\n",
      "Training acc: 0.97, Training loss: 0.13, Validation acc: 0.96, Validation loss: 0.15\n",
      "Training acc: 0.97, Training loss: 0.11, Validation acc: 0.96, Validation loss: 0.16\n",
      "Training acc: 0.97, Training loss: 0.14, Validation acc: 0.96, Validation loss: 0.15\n",
      "Training acc: 0.96, Training loss: 0.13, Validation acc: 0.96, Validation loss: 0.13\n",
      "0.01 accuracy change. Early stopping...\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 7\n",
    "EVALUATION_FREQ = len(training_set) // batch_size // 10  # guarantee 10 evaluations per epoch\n",
    "\n",
    "model.train(mode=True)  # just puts the model in training mode (doesn't actually train)\n",
    "\n",
    "training_acc_lst, training_loss_lst = [], []\n",
    "validation_acc_lst, validation_loss_lst = [], []\n",
    "epochs_acc = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    epoch_acc = []\n",
    "    training_acc_checkpoint, training_loss_checkpoint = [], []\n",
    "    for batch_idx, (data, labels) in enumerate(training_loader):\n",
    "        # cast to device (cpu / gpu) (gpu for faster op). also both need to be on same device.\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        # run model on data to get predictions.\n",
    "        # as we saw earlier, training_set[i][0] is of shape (1,28,28),\n",
    "        # that means data is of shape (batch_size, 1, 28, 28),\n",
    "        # but pytorch models require each sample to be 1 dimensional, so we need to flatten the data to (batch_size, 28*28) (done in evaluate())\n",
    "        predictions, acc, loss = evaluate(model, loss_function, data, labels)\n",
    "        training_acc_checkpoint.append(acc)\n",
    "        epoch_acc.append(acc)\n",
    "\n",
    "        # loss already calculated in the evaluate() call. just append it\n",
    "        training_loss_checkpoint.append(loss.item())\n",
    "        \n",
    "        # back propagation (calculate the gradient)\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent (adjust the weights)\n",
    "        optimizer.step()\n",
    "\n",
    "        # default behavior of pytorch is to NOT clear the gradients after every step.\n",
    "        # but we need to clear them to prevent accumulation of gradients throughout iterations.\n",
    "        optimizer.zero_grad()  # or model.zero_grad() if all the model's parameters are in the optimizer (in our case they are)\n",
    "\n",
    "        # evaluate on validation\n",
    "        if batch_idx % EVALUATION_FREQ == 0:\n",
    "            # average training acc and loss every EVALUATION_FREQ, so our training and validation plots axes will have the same length\n",
    "            training_acc_lst.append(np.mean(training_acc_checkpoint))\n",
    "            training_loss_lst.append(np.mean(training_loss_checkpoint))\n",
    "            # restart checkpoints\n",
    "            training_acc_checkpoint, training_loss_checkpoint = [], []\n",
    "\n",
    "            # predict validation data, but first disable gradient tracking, and enter evaluation mode\n",
    "            model.train(mode=False)  # enter eval mode. suggested here: https://stackoverflow.com/a/55627781/900394\n",
    "            with torch.no_grad():  # locally disable gradient tracking\n",
    "                validation_acc_checkpoint, validation_loss_checkpoint = [], []\n",
    "                validation_predictions = []  # saved for showing results later\n",
    "                for val_batch_idx, (val_data, val_labels) in enumerate(validation_loader):\n",
    "                    val_data, val_labels = val_data.to(device), val_labels.to(device)\n",
    "\n",
    "                    val_predictions, validation_acc, validation_loss = evaluate(model, loss_function, val_data, val_labels)\n",
    "                    \n",
    "                    validation_loss_checkpoint.append(validation_loss.item())\n",
    "                    validation_acc_checkpoint.append(validation_acc)\n",
    "                    validation_predictions.extend(val_predictions)  # predictions are for a complete batch, so we need to \"extend\" not \"append\"\n",
    "                \n",
    "                validation_acc_lst.append(np.mean(validation_acc_checkpoint))\n",
    "                validation_loss_lst.append(np.mean(validation_loss_checkpoint))\n",
    "            \n",
    "            print(f'Training acc: {training_acc_lst[-1]:.2f}, Training loss: {training_loss_lst[-1]:.2f}, Validation acc: {validation_acc_lst[-1]:.2f}, Validation loss: {validation_loss_lst[-1]:.2f}')\n",
    "\n",
    "            model.train(mode=True)  # re-enter training mode\n",
    "\n",
    "    # epoch end\n",
    "    epochs_acc.append(np.mean(epoch_acc))\n",
    "    # early stopping according to accuracy\n",
    "    # TODO: allow K consecutive and / or take validation acc into account\n",
    "    if len(epochs_acc) > 1 and epochs_acc[-1] - epochs_acc[-2] < 0.01:\n",
    "        print(f'{epochs_acc[-1] - epochs_acc[-2]:.2f} accuracy change. Early stopping...')\n",
    "        break\n",
    "\n",
    "junk = model.train(mode=False)  # exit training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97, Loss: 0.13\n"
     ]
    }
   ],
   "source": [
    "model.train(False)  # ensure we're in eval mode\n",
    "\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "for X,y in test_loader:\n",
    "    with torch.no_grad():\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        predictions = model(X.reshape(X.shape[0], -1))\n",
    "\n",
    "        loss = loss_function(predictions, y)\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "        test_acc.append((predictions.argmax(dim=1).cpu().numpy() == y.cpu().numpy()).mean())\n",
    "\n",
    "print(f'Accuracy: {np.mean(test_acc):.2f}, Loss: {np.mean(test_loss):.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
